{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "230e897a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "import random\n",
    "import pandas as pd\n",
    "from torch_geometric.data import Data, Dataset, DataLoader\n",
    "import tqdm\n",
    "import glob\n",
    "import pandas\n",
    "import torch\n",
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "import random\n",
    "import plotly.graph_objects as go\n",
    "from torch_geometric.data import Data, Dataset, DataLoader\n",
    "import matplotlib._color_data as mcd\n",
    "\"libraries for debugging\"\n",
    "import sys\n",
    "import os.path as osp\n",
    "\"custom imports\"\n",
    "from graphconv import GravnetModel\n",
    "from optimized_oc import calc_LV_Lbeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "987d2c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColorWheel:\n",
    "    '''Returns a consistent color when given the same object'''\n",
    "    def __init__(self, colors=None, seed=44):\n",
    "        if colors is None:\n",
    "            self.colors = list(mcd.XKCD_COLORS.values())\n",
    "        else:\n",
    "            self.colors = colors\n",
    "        np.random.seed(seed)\n",
    "        np.random.shuffle(self.colors)\n",
    "        self._original_colors = self.colors.copy()\n",
    "        self.assigned_colors = {}\n",
    "        \n",
    "    def __call__(self, thing):\n",
    "        key = id(thing)\n",
    "        if key in self.assigned_colors:\n",
    "            return self.assigned_colors[key]\n",
    "        else:\n",
    "            color = self.colors.pop()\n",
    "            self.assigned_colors[key] = color\n",
    "            if not(self.colors): self.colors = self._original_colors.copy()\n",
    "            return color\n",
    "    \n",
    "    def assign(self, thing, color):\n",
    "        \"\"\"Assigns a specific color to a thing\"\"\"\n",
    "        key = id(thing)\n",
    "        self.assigned_colors[key] = color\n",
    "        if color in self.colors: self.colors.remove(color)\n",
    "\n",
    "\n",
    "def get_plotly_clusterspace(event, \n",
    "                            cluster_space_coords, \n",
    "                            clustering=None,\n",
    "                            size = 1.00):\n",
    "    assert cluster_space_coords.size(1) == 3\n",
    "    import plotly.graph_objects as go\n",
    "\n",
    "    colorwheel = ColorWheel()\n",
    "    colorwheel.assign(0, '#bfbfbf')\n",
    "    colorwheel.assign(-1, '#bfbfbf')\n",
    "\n",
    "    data = []\n",
    "\n",
    "    if clustering is None: clustering = event.y\n",
    "\n",
    "    for cluster_index in np.unique(clustering):\n",
    "        x = cluster_space_coords[clustering == cluster_index].numpy()\n",
    "        data.append(go.Scatter3d(\n",
    "            x=x[:,0], y=x[:,1], z=x[:,2],\n",
    "            mode='markers', \n",
    "            marker=dict(\n",
    "                line=dict(width=0),\n",
    "                size=size,\n",
    "                color= colorwheel(int(cluster_index)),\n",
    "                ),\n",
    "            hovertemplate=(\n",
    "                f'x=%{{y:0.2f}}<br>y=%{{z:0.2f}}<br>z=%{{x:0.2f}}'\n",
    "                f'<br>clusterindex={cluster_index}'\n",
    "                f'<br>'\n",
    "                ),\n",
    "            name = f'cluster_{cluster_index}'\n",
    "            ))\n",
    "    return data\n",
    "\n",
    "def get_plotly_truth(event,\n",
    "                    size = 1.00):\n",
    "    colorwheel = ColorWheel()\n",
    "    colorwheel.assign(0, '#bfbfbf')\n",
    "\n",
    "    data = []\n",
    "\n",
    "    for cluster_index in np.unique(event.y):\n",
    "        x = event.x[event.y == cluster_index].numpy()\n",
    "        data.append(go.Scatter3d(\n",
    "            x=x[:,0], y=x[:,1], z=x[:,2],\n",
    "#             x=x[:,3],y=x[:,-1],z=x[:,2],\n",
    "            mode='lines+markers', \n",
    "            marker=dict(\n",
    "                line=dict(width=0),\n",
    "                size=size,\n",
    "                color= colorwheel(int(cluster_index)),\n",
    "                ),\n",
    "            hovertemplate=(\n",
    "                f'x=%{{y:0.2f}}<br>y=%{{z:0.2f}}<br>z=%{{x:0.2f}}'\n",
    "                f'<br>clusterindex={cluster_index}'\n",
    "                f'<br>'\n",
    "                ),\n",
    "            name = f'cluster_{cluster_index}'\n",
    "            ))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20f9c2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrackMLParticleTrackingDataset(Dataset):\n",
    "    def __init__(self, root, \n",
    "                 transform=None, \n",
    "                 n_events=0,\n",
    "                 directed=False, \n",
    "                 layer_pairs_plus=False,\n",
    "                 volume_layer_ids=[[8, 2], [8, 4], [8, 6], [8, 8]], #Layers Selecte\n",
    "                 layer_pairs=[[7, 8], [8, 9], [9, 10]],             #Connected Layers\n",
    "                 pt_min=2.0, \n",
    "                 eta_range=[-5, 5],                     \n",
    "                 phi_slope_max=0.0006, \n",
    "                 z0_max=150,                  \n",
    "                 n_phi_sections=1, \n",
    "                 n_eta_sections=1,  \n",
    "                 augments = False,\n",
    "                 tracking=False,                   \n",
    "                 n_workers=mp.cpu_count(), \n",
    "                 n_tasks=1,               \n",
    "                 download_full_dataset=False                        \n",
    "             ):\n",
    "        hits = glob.glob(osp.join(osp.join(root,'raw'), 'event*-hits.csv'))\n",
    "        self.hits = sorted(hits)\n",
    "        particles = glob.glob(osp.join(osp.join(root,'raw'), 'event*-particles.csv'))\n",
    "        self.particles = sorted(particles)\n",
    "        truth = glob.glob(osp.join(osp.join(root,'raw'), 'event*-truth.csv'))\n",
    "        self.truth = sorted(truth)\n",
    "        if (n_events > 0):\n",
    "            self.hits = self.hits[:n_events]\n",
    "            self.particles = self.particles[:n_events]\n",
    "            self.truth = self.truth[:n_events]\n",
    "        self.layer_pairs_plus = layer_pairs_plus\n",
    "        self.volume_layer_ids = torch.tensor(volume_layer_ids)\n",
    "        self.layer_pairs      = torch.tensor(layer_pairs)\n",
    "        self.pt_min           = pt_min\n",
    "        self.eta_range        = eta_range\n",
    "        self.n_phi_sections   = n_phi_sections\n",
    "        self.n_eta_sections   = n_eta_sections\n",
    "        self.full_dataset     = download_full_dataset\n",
    "        self.n_events         = n_events\n",
    "\n",
    "#         self.phi_slope_max    = phi_slope_max\n",
    "#         self.z0_max           = z0_max\n",
    "#         self.augments         = augments\n",
    "#         self.tracking         = tracking\n",
    "#         self.n_tasks          = n_tasks\n",
    "\n",
    "        super(TrackMLParticleTrackingDataset, self).__init__(root, transform)\n",
    "\n",
    "    def len(self):\n",
    "        N_events = len(self.hits)\n",
    "        return N_events*self.n_phi_sections*self.n_eta_sections\n",
    "\n",
    "    def __len__(self):\n",
    "        N_events = len(self.hits)\n",
    "        return N_events*self.n_phi_sections*self.n_eta_sections\n",
    "\n",
    "    def read_events(self,idx):\n",
    "        hits_filename = self.hits[idx]\n",
    "        hits = pandas.read_csv(\n",
    "            hits_filename, usecols=['hit_id', 'x', 'y', 'z', 'volume_id', 'layer_id', 'module_id'],\n",
    "            dtype={\n",
    "                'hit_id': np.int64,\n",
    "                'x': np.float32,\n",
    "                'y': np.float32,\n",
    "                'z': np.float32,\n",
    "                'volume_id': np.int64,\n",
    "                'layer_id': np.int64,\n",
    "                'module_id': np.int64\n",
    "            })\n",
    "        particles_filename = self.particles[idx]\n",
    "        particles = pandas.read_csv(\n",
    "            particles_filename, usecols=['particle_id', 'vx', 'vy', 'vz', 'px', 'py', 'pz', 'q', 'nhits'],\n",
    "            dtype={\n",
    "                'particle_id': np.int64,\n",
    "                'vx': np.float32,\n",
    "                'vy': np.float32,\n",
    "                'vz': np.float32,\n",
    "                'px': np.float32,\n",
    "                'py': np.float32,\n",
    "                'pz': np.float32,\n",
    "                'q': np.int64,\n",
    "                'nhits': np.int64\n",
    "            })\n",
    "        truth_filename = self.truth[idx]\n",
    "        truth = pandas.read_csv(\n",
    "            truth_filename, usecols=['hit_id', 'particle_id', 'tx', 'ty', 'tz', 'tpx', 'tpy', 'tpz', 'weight'],\n",
    "            dtype={\n",
    "                'hit_id': np.int64,\n",
    "                'particle_id': np.int64,\n",
    "                'tx': np.float32,\n",
    "                'ty': np.float32,\n",
    "                'tz': np.float32,\n",
    "                'tpx': np.float32,\n",
    "                'tpy': np.float32,\n",
    "                'tpz': np.float32,\n",
    "                'weight': np.float32\n",
    "            })\n",
    "        return hits,particles,truth\n",
    "\n",
    "#     def select_hits(self, hits,particles,truth, noise_label = -1):\n",
    "\n",
    "#         valid_layer = 20 * self.volume_layer_ids[:,0] + self.volume_layer_ids[:,1]\n",
    "#         n_det_layers = len(valid_layer)\n",
    "\n",
    "#         # Add markers for valid layers and corresponding indices to the hits\n",
    "#         layer = torch.from_numpy(20 * hits['volume_id'].values + hits['layer_id'].values)\n",
    "#         index = layer.unique(return_inverse=True)[1]\n",
    "#         hits = hits[['hit_id', 'x', 'y', 'z']].assign(layer=layer, index=index)\n",
    "#         valid_groups = hits.groupby(['layer'])\n",
    "#         hits = pd.concat([valid_groups.get_group(valid_layer.numpy()[i]) for i in range(n_det_layers)])\n",
    "\n",
    "#         hits = (hits[['hit_id', 'x', 'y', 'z', 'index']].merge(truth[['hit_id', 'particle_id']], on='hit_id'))\n",
    "#         hits['track_id'] = hits['hit_id'].astype(str) + \"-\" + hits['particle_id'].astype(str)\n",
    "\n",
    "#         # Compute other characteristics for the hits [r,phi, theta, eta]\n",
    "#         r = np.sqrt(hits['x'].values**2 + hits['y'].values**2)\n",
    "#         phi = np.arctan2(hits['y'].values, hits['x'].values)\n",
    "#         theta = np.arctan2(r,hits['z'].values)\n",
    "#         eta = -1*np.log(np.tan(theta/2))\n",
    "#         hits = hits[['track_id','x','y','z', 'index', 'particle_id']].assign(r=r, \n",
    "#                                                                          phi=phi, \n",
    "#                                                                          eta=eta, \n",
    "#                                                                          theta = theta)\n",
    "#         # Computing the counts of tracks/ hits associated with each particle \n",
    "#         hit_counts = hits.groupby(by=['particle_id']).size().reset_index(name='counts')\n",
    "#         hits = hits.merge(hit_counts[['counts','particle_id']],on='particle_id')\n",
    "\n",
    "#         # get the noisy hits in a separate dataframe and assign them noise_label (cluster label for noise)\n",
    "#         noise_hits = hits[hits.particle_id == 0]\n",
    "#         if noise_label != 0:\n",
    "#             noise_hits.replace(0,noise_label)\n",
    "#         noise_hits.insert(0, 'pt', 0.0) # add pt = 0.0 as a nominal column \n",
    "\n",
    "#         # Compute pt for the particles\n",
    "#         particles['pt'] = np.sqrt(particles['px']**2 + particles['py']**2)\n",
    "\n",
    "#         # Merge hits with particles to select only valid particles\n",
    "#         hits = hits.merge(particles[['particle_id','pt']], on='particle_id')\n",
    "\n",
    "#         selected_hits = pd.concat([hits,noise_hits])\n",
    "\n",
    "#         # Mark all the particles where associated hit counts < 2 or pt <= pt_min as noise\n",
    "#         selected_hits['particle_id'].where((selected_hits['counts'] >= 2) & (selected_hits['pt'] > self.pt_min),noise_label,inplace=True)\n",
    "\n",
    "#         # Compute the remapped ids to make the labels contiguous\n",
    "#         pids_unique, pids_inverse, _ = np.unique(selected_hits['particle_id'].values, return_inverse=True, return_counts=True)  \n",
    "#         pids_unique = np.arange(pids_unique.size) \n",
    "#         selected_hits['remapped_pid'] = pids_unique[pids_inverse]\n",
    "\n",
    "#         selected_ids = [0,1,2,3,4]\n",
    "#         selected_hits = selected_hits[selected_hits.remapped_pid.isin(selected_ids)]\n",
    "        \n",
    "#         # Extract the features from the selected_hits\n",
    "#         x = torch.from_numpy(selected_hits['x'].values)\n",
    "#         y = torch.from_numpy(selected_hits['y'].values)\n",
    "#         theta = torch.from_numpy(selected_hits['theta'].values)\n",
    "#         r = torch.from_numpy(selected_hits['r'].values)\n",
    "#         phi = torch.from_numpy(selected_hits['phi'].values)\n",
    "#         z = torch.from_numpy(selected_hits['z'].values)\n",
    "#         eta = torch.from_numpy(selected_hits['eta'].values)\n",
    "#     #    layer = torch.from_numpy(selected_hits['index'].values)\n",
    "#     #    particle = torch.from_numpy(selected_hits['particle_id'].values)\n",
    "#         particle_labels = torch.from_numpy(selected_hits['remapped_pid'].values)\n",
    "#         # Select only a set of labels for testing\n",
    "\n",
    "#         pos = torch.stack([x, y, z, r, theta, phi], 1)   \n",
    "\n",
    "#        return pos, eta, particle_labels\n",
    "\n",
    "    def select_all_hits(self,hits,particles,truth, noise_label = -1):\n",
    "        valid_layer = 20 * self.volume_layer_ids[:,0] + self.volume_layer_ids[:,1]\n",
    "        n_det_layers = len(valid_layer)\n",
    "\n",
    "        # Add markers for valid layers and corresponding indices to the hits\n",
    "        layer = torch.from_numpy(20 * hits['volume_id'].values + hits['layer_id'].values)\n",
    "        index = layer.unique(return_inverse=True)[1]\n",
    "        hits = hits[['hit_id', 'x', 'y', 'z']].assign(layer=layer, index=index)\n",
    "        valid_groups = hits.groupby(['layer'])\n",
    "        hits = pd.concat([valid_groups.get_group(valid_layer.numpy()[i]) for i in range(n_det_layers)])\n",
    "        hits = (hits[['hit_id', 'x', 'y', 'z', 'index']].merge(truth[['hit_id', 'particle_id']], on='hit_id'))\n",
    "        hits['track_id'] = hits['hit_id'].astype(str) + \"-\" + hits['particle_id'].astype(str)\n",
    "        # Compute other characteristics for the hits [r,phi, theta, eta]\n",
    "        r = np.sqrt(hits['x'].values**2 + hits['y'].values**2)\n",
    "        phi = np.arctan2(hits['y'].values, hits['x'].values)\n",
    "        theta = np.arctan2(r,hits['z'].values)\n",
    "        eta = -1*np.log(np.tan(theta/2))\n",
    "        hits = hits[['track_id','x','y','z', 'index', 'particle_id']].assign(r=r, \n",
    "                                                                         phi=phi, \n",
    "                                                                         eta=eta, \n",
    "                                                                         theta = theta)\n",
    "#         NO LONGER NEEDED TO COMPUTE COUNT; NOISE IDENTIFIED BY nhits\n",
    "#         hit_counts = hits.groupby(by=['particle_id']).size().reset_index(name='counts')\n",
    "#         hits = hits.merge(hit_counts[['counts','particle_id']],on='particle_id')\n",
    "\n",
    "        # Compute pt for the particles\n",
    "        particles['pt'] = np.sqrt(particles['px']**2 + particles['py']**2)\n",
    "\n",
    "        # Merge hits with particles keeping all entities in the hits table\n",
    "        selected_hits = hits.merge(particles[['particle_id','pt','nhits']], on='particle_id', how = \"left\")\n",
    "        selected_hits['nhits']=selected_hits['nhits'].fillna(0)\n",
    "        selected_hits['pt']=selected_hits['pt'].fillna(0)\n",
    "\n",
    "        # Mark all the particles where associated (nhits < 2) AND (pt <= pt_min) as noise\n",
    "        selected_hits['particle_id'].where((selected_hits['nhits'] >= 2) & (selected_hits['pt'] > self.pt_min),noise_label,inplace=True)\n",
    "\n",
    "        # Compute the remapped ids to make the labels contiguous\n",
    "        pids_unique, pids_inverse, _ = np.unique(selected_hits['particle_id'].values, return_inverse=True, return_counts=True)  \n",
    "        pids_unique = np.arange(pids_unique.size) \n",
    "        selected_hits['remapped_pid'] = pids_unique[pids_inverse]\n",
    "\n",
    "        # Select a subset noise + tracks to be selected as signals\n",
    "        noise = selected_hits[selected_hits.remapped_pid == 0]\n",
    "        idx = random.sample(noise.index.to_list(),50)\n",
    "        idx.sort()\n",
    "        noise = noise.loc[idx]\n",
    "        \n",
    "        signal_tracks = [1,2,3,4]\n",
    "        signals = selected_hits[selected_hits.remapped_pid.isin(signal_tracks)]\n",
    "        \n",
    "        selected_hits = pd.concat([signals,noise])\n",
    "\n",
    "        # Extract the features from the selected_hits\n",
    "        x = torch.from_numpy(selected_hits['x'].values)\n",
    "        y = torch.from_numpy(selected_hits['y'].values)\n",
    "        theta = torch.from_numpy(selected_hits['theta'].values)\n",
    "        r = torch.from_numpy(selected_hits['r'].values)\n",
    "        phi = torch.from_numpy(selected_hits['phi'].values)\n",
    "        z = torch.from_numpy(selected_hits['z'].values)\n",
    "        eta = torch.from_numpy(selected_hits['eta'].values)\n",
    "        particle_labels = torch.from_numpy(selected_hits['remapped_pid'].values)\n",
    "        pos = torch.stack([x, y, z, r, theta, phi], 1)   \n",
    "\n",
    "        return pos, eta, particle_labels\n",
    "    \n",
    "    def split_detector_sections(self,pos, eta, particle_labels, phi_edges, eta_edges):\n",
    "        pos_sect, particle_label_sect = [], []\n",
    "        # Refer to the index of the column representing phi values in pos tensor\n",
    "        phi_idx = -1\n",
    "        for i in range(len(phi_edges) - 1):\n",
    "            phi_mask1 = pos[:,phi_idx] > phi_edges[i]\n",
    "            phi_mask2 = pos[:,phi_idx] < phi_edges[i+1]\n",
    "            phi_mask  = phi_mask1 & phi_mask2\n",
    "            phi_pos      = pos[phi_mask]\n",
    "            phi_eta      = eta[phi_mask]\n",
    "            phi_particle_label = particle_labels[phi_mask]\n",
    "\n",
    "            for j in range(len(eta_edges) - 1):\n",
    "                eta_mask1 = phi_eta > eta_edges[j]\n",
    "                eta_mask2 = phi_eta < eta_edges[j+1]\n",
    "                eta_mask  = eta_mask1 & eta_mask2\n",
    "                phi_eta_pos = phi_pos[eta_mask]\n",
    "                phi_eta_particle_label = phi_particle_label[eta_mask]\n",
    "                pos_sect.append(phi_eta_pos)\n",
    "                particle_label_sect.append(phi_eta_particle_label)\n",
    "\n",
    "        return pos_sect, particle_label_sect\n",
    "    \n",
    "    def get(self,idx):\n",
    "        \n",
    "        hits,particles,truth = self.read_events(idx)   \n",
    "        pos, eta, particle_labels = self.select_all_hits(hits, \n",
    "                                                         particles, \n",
    "                                                         truth,noise_label=0)\n",
    "        tracks = torch.empty(0, 5, dtype=torch.long)  \n",
    "        phi_edges = np.linspace(*(-np.pi, np.pi), num=self.n_phi_sections+1)\n",
    "        eta_edges = np.linspace(*self.eta_range, num=self.n_eta_sections+1)\n",
    "        pos_sect, particle_label_sect = self.split_detector_sections(pos, \n",
    "                                                                    eta,\n",
    "                                                                    particle_labels, \n",
    "                                                                    phi_edges, \n",
    "                                                                    eta_edges)\n",
    "        for i in range(len(pos_sect)):\n",
    "            y = particle_label_sect[0]\n",
    "            return Data(x=pos_sect[0],\n",
    "                        y=y,\n",
    "                        tracks=tracks,\n",
    "                        inpz = torch.Tensor([i]))\n",
    "\n",
    "def fetch_dataloader(data_dir, \n",
    "                     batch_size, \n",
    "                     validation_split,\n",
    "                     n_events = 100,\n",
    "                     pt_min = 1.0,\n",
    "                     n_workers = 1,\n",
    "                     generate_tracks = True,\n",
    "                     full_dataset = False,\n",
    "                     shuffle=False):\n",
    "    volume_layer_ids = [\n",
    "        [8, 2], [8, 4], [8, 6], [8, 8], # barrel pixels\n",
    "        [7, 2], [7, 4], [7, 6], [7, 8], [7, 10], [7, 12], [7, 14],# minus pixel endcap\n",
    "        [9, 2], [9, 4], [9, 6], [9, 8], [9, 10], [9, 12], [9, 14], # plus pixel endcap\n",
    "    ]\n",
    "    dataset = TrackMLParticleTrackingDataset(root=data_dir,\n",
    "                                             layer_pairs_plus=True, \n",
    "                                             pt_min= pt_min,\n",
    "                                             volume_layer_ids=volume_layer_ids,\n",
    "                                             n_events=n_events, \n",
    "                                             n_workers=n_workers, \n",
    "                                             tracking = generate_tracks,\n",
    "                                             download_full_dataset=full_dataset)\n",
    "    dataset_size = len(dataset)\n",
    "    indices = list(range(dataset_size))\n",
    "    if dataset_size > 2:\n",
    "        split = int(np.floor(validation_split * dataset_size))\n",
    "    else: \n",
    "        split = 1\n",
    "    print(split)\n",
    "    random_seed= 1001\n",
    "\n",
    "    train_subset, val_subset = torch.utils.data.random_split(dataset, [dataset_size - split, split],\n",
    "                                                             generator=torch.Generator().manual_seed(random_seed))\n",
    "    print(\"train subset dim:\", len(train_subset))\n",
    "    print(\"validation subset dim\", len(val_subset))\n",
    "    dataloaders = {\n",
    "        'train':  DataLoader(train_subset, batch_size=batch_size, shuffle=shuffle),\n",
    "        'val':   DataLoader(val_subset, batch_size=batch_size, shuffle=shuffle)\n",
    "        }\n",
    "    print(\"train_dataloader dim:\", len(dataloaders['train']))\n",
    "    print(\"val dataloader dim:\", len(dataloaders['val']))\n",
    "    return dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f98efb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_offset: float = 1.0\n",
    "\n",
    "def compute_oc_loss(out, data, s_c=1., return_components=False):\n",
    "    device = out.device\n",
    "    pred_betas = torch.sigmoid(out[:,0])\n",
    "    pred_cluster_space_coords = out[:,1:4]\n",
    "    assert all(t.device == device for t in [\n",
    "        pred_betas, pred_cluster_space_coords, data.y,\n",
    "        data.batch,\n",
    "        ])\n",
    "    out_oc = calc_LV_Lbeta(\n",
    "        pred_betas,\n",
    "        pred_cluster_space_coords,\n",
    "        data.y.long(),\n",
    "        data.batch,\n",
    "        return_components=return_components,\n",
    "        qmin = 0.1\n",
    "        )\n",
    "    if return_components:\n",
    "        return out_oc\n",
    "    else:\n",
    "        LV, Lbeta = out_oc\n",
    "        return LV + Lbeta + loss_offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4dc35b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data_loader, model, epoch, optimizer,interval = 1):\n",
    "    print('Training epoch', epoch)\n",
    "    model.train()\n",
    "    data = tqdm.tqdm(data_loader, total=len(data_loader))\n",
    "    data.set_postfix({'loss': '?'})\n",
    "    for i,inputs in enumerate(data):\n",
    "        optimizer.zero_grad()\n",
    "        result = model(inputs.x, inputs.batch)\n",
    "        loss = compute_oc_loss(result,inputs)\n",
    "        if i % interval == 0:\n",
    "            print(f'loss={float(loss)}')\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        data.set_postfix({'loss': float(loss)})\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8159f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(data_loader, model, epoch, generate_plots = False):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        loss = 0.\n",
    "        data = tqdm.tqdm(data_loader, total=len(data_loader))\n",
    "        for i,inputs in enumerate(data):\n",
    "            result = model(inputs.x, inputs.batch)\n",
    "            loss +=  compute_oc_loss(result,inputs)\n",
    "            pred_betas = torch.sigmoid(result[:,0])\n",
    "            pred_cluster_space_coords = result[:,1:4]\n",
    "            if generate_plots:\n",
    "                fig = go.Figure(get_plotly_truth(inputs,size = 2.75))\n",
    "                fig.write_html(\"plots/truth_plot_epoch_\"+str(epoch+1)+\"_batch_\"+str(i+1)+\".html\")\n",
    "                pred_fig=go.Figure(get_plotly_clusterspace(inputs,pred_cluster_space_coords,size = 2.75))\n",
    "                pred_fig.write_html(\"plots/predictions_plot_epoch_\"+str(epoch+1)+\"_batch_\"+str(i+1)+\".html\")\n",
    "        loss /= len(data_loader)\n",
    "        print(f'Avg test loss: {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f84894",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9 [00:00<?, ?it/s, loss=?]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "train subset dim: 45\n",
      "validation subset dim 5\n",
      "train_dataloader dim: 9\n",
      "val dataloader dim: 1\n",
      "Training epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 1/9 [00:02<00:23,  2.99s/it, loss=2.14]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=2.1412487030029297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 6/9 [00:17<00:08,  2.89s/it, loss=2.14]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=2.1429831981658936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:25<00:00,  2.85s/it, loss=2.14]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.73s/it]\n",
      "  0%|          | 0/9 [00:00<?, ?it/s, loss=?]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg test loss: 2.135890007019043\n",
      "Training epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 1/9 [00:02<00:20,  2.61s/it, loss=2.14]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=2.1367688179016113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 6/9 [00:16<00:08,  2.84s/it, loss=2.15]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=2.1478421688079834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:25<00:00,  2.86s/it, loss=2.12]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.72s/it]\n",
      "  0%|          | 0/9 [00:00<?, ?it/s, loss=?]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg test loss: 2.15014910697937\n",
      "Training epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 1/9 [00:02<00:23,  2.95s/it, loss=2.13]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=2.1260814666748047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 6/9 [00:17<00:08,  2.89s/it, loss=2.13]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=2.127979278564453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:25<00:00,  2.87s/it, loss=2.13]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.82s/it]\n",
      "  0%|          | 0/9 [00:00<?, ?it/s, loss=?]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg test loss: 2.1249098777770996\n",
      "Training epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 1/9 [00:03<00:24,  3.01s/it, loss=2.12]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=2.119096279144287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 6/9 [00:17<00:08,  2.86s/it, loss=2.12]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=2.121321201324463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:26<00:00,  2.89s/it, loss=2.13]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.75s/it]\n",
      "  0%|          | 0/9 [00:00<?, ?it/s, loss=?]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg test loss: 2.112483024597168\n",
      "Training epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 1/9 [00:02<00:22,  2.81s/it, loss=2.12]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=2.1171886920928955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 6/9 [00:17<00:08,  2.93s/it, loss=2.11]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=2.113752841949463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:26<00:00,  2.92s/it, loss=2.11]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.74s/it]\n",
      "  0%|          | 0/9 [00:00<?, ?it/s, loss=?]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg test loss: 2.1321463584899902\n",
      "Training epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 1/9 [00:02<00:23,  2.98s/it, loss=2.1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=2.102365255355835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 6/9 [00:17<00:08,  2.93s/it, loss=2.11]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=2.1095707416534424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:26<00:00,  2.90s/it, loss=2.11]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.85s/it]\n",
      "  0%|          | 0/9 [00:00<?, ?it/s, loss=?]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg test loss: 2.1477556228637695\n",
      "Training epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 1/9 [00:02<00:21,  2.69s/it, loss=2.09]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=2.0905303955078125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 6/9 [00:17<00:08,  2.99s/it, loss=2.1] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=2.1017212867736816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:26<00:00,  2.91s/it, loss=2.11]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.81s/it]\n",
      "  0%|          | 0/9 [00:00<?, ?it/s, loss=?]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg test loss: 2.1337742805480957\n",
      "Training epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 1/9 [00:02<00:22,  2.84s/it, loss=2.09]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=2.090026378631592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 6/9 [00:17<00:08,  2.92s/it, loss=2.1] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=2.0956039428710938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:26<00:00,  2.91s/it, loss=2.1] \n",
      "100%|██████████| 1/1 [00:02<00:00,  2.82s/it]\n",
      "  0%|          | 0/9 [00:00<?, ?it/s, loss=?]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg test loss: 2.1030995845794678\n",
      "Training epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 1/9 [00:02<00:23,  2.98s/it, loss=2.09]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=2.0941643714904785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 6/9 [00:17<00:08,  2.98s/it, loss=2.07]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=2.072295665740967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:26<00:00,  2.91s/it, loss=2.08]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.85s/it]\n",
      "  0%|          | 0/9 [00:00<?, ?it/s, loss=?]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg test loss: 2.0721447467803955\n",
      "Training epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 1/9 [00:03<00:24,  3.07s/it, loss=2.04]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=2.043281078338623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 6/9 [00:17<00:08,  2.86s/it, loss=2.09]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=2.0894367694854736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:26<00:00,  2.90s/it, loss=2.08]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.76s/it]\n",
      "  0%|          | 0/9 [00:00<?, ?it/s, loss=?]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg test loss: 2.1052756309509277\n",
      "Training epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 1/9 [00:02<00:23,  2.92s/it, loss=2.08]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=2.0843417644500732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 6/9 [00:17<00:08,  2.93s/it, loss=2.03]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=2.0260260105133057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:26<00:00,  2.92s/it, loss=2.08]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.74s/it]\n",
      "  0%|          | 0/9 [00:00<?, ?it/s, loss=?]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg test loss: 2.0747928619384766\n",
      "Training epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 1/9 [00:02<00:23,  2.92s/it, loss=2.07]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=2.0695037841796875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 6/9 [00:17<00:08,  2.87s/it, loss=2.04]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=2.0409457683563232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:26<00:00,  2.93s/it, loss=2.03]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.76s/it]\n",
      "  0%|          | 0/9 [00:00<?, ?it/s, loss=?]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg test loss: 2.098924160003662\n",
      "Training epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 1/9 [00:02<00:21,  2.72s/it, loss=2.02]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=2.0245842933654785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 6/9 [00:17<00:08,  2.96s/it, loss=2.02]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=2.022092342376709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:26<00:00,  2.95s/it, loss=2.02]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.84s/it]\n",
      "  0%|          | 0/9 [00:00<?, ?it/s, loss=?]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg test loss: 2.0523979663848877\n",
      "Training epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 1/9 [00:03<00:25,  3.22s/it, loss=2.03]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=2.033064365386963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 6/9 [00:17<00:08,  2.99s/it, loss=2.03]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=2.025028705596924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 8/9 [00:23<00:02,  2.88s/it, loss=2.04]"
     ]
    }
   ],
   "source": [
    "root = \"/Users/gpradhan/Downloads/train_1\"\n",
    "noise_pt_min = 0.0\n",
    "batch_size = 5\n",
    "validation_split = 0.1\n",
    "events = 50\n",
    "model_input_dim = 6\n",
    "model_output_dim = 4\n",
    "data = fetch_dataloader(data_dir = root,\n",
    "                        batch_size = batch_size,\n",
    "                        validation_split=validation_split,\n",
    "                        full_dataset = False,\n",
    "                        n_events = events,\n",
    "                        pt_min = noise_pt_min,\n",
    "                        shuffle=True)\n",
    "epochs = 20\n",
    "train_loader,test_loader = data['train'],data['val']\n",
    "model = GravnetModel(input_dim=6,output_dim=4)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, weight_decay=1e-4)\n",
    "\n",
    "for i_epoch in range(epochs):\n",
    "    model = train(train_loader, \n",
    "                  model, \n",
    "                  i_epoch, \n",
    "                  optimizer,\n",
    "                 interval = 5)\n",
    "    test(test_loader, \n",
    "         model, \n",
    "         i_epoch,\n",
    "        generate_plots = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcef8eb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5207d851",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### END ############################################################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
