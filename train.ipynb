{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230e897a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "from torch_geometric.data import Data, Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "import pandas\n",
    "import torch\n",
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "import random\n",
    "import plotly.graph_objects as go\n",
    "from torch_geometric.data import Data, Dataset, DataLoader\n",
    "import matplotlib._color_data as mcd\n",
    "\"libraries for debugging\"\n",
    "import sys\n",
    "import os.path as osp\n",
    "\"custom imports\"\n",
    "from torch_cmspepr.gravnet_model import GravnetModel\n",
    "from torch_cmspepr.objectcondensation import calc_LV_Lbeta, formatted_loss_components_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987d2c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColorWheel:\n",
    "    '''Returns a consistent color when given the same object'''\n",
    "    def __init__(self, colors=None, seed=44):\n",
    "        if colors is None:\n",
    "            self.colors = list(mcd.XKCD_COLORS.values())\n",
    "        else:\n",
    "            self.colors = colors\n",
    "        np.random.seed(seed)\n",
    "        np.random.shuffle(self.colors)\n",
    "        self._original_colors = self.colors.copy()\n",
    "        self.assigned_colors = {}\n",
    "        \n",
    "    def __call__(self, thing):\n",
    "        key = id(thing)\n",
    "        if key in self.assigned_colors:\n",
    "            return self.assigned_colors[key]\n",
    "        else:\n",
    "            color = self.colors.pop()\n",
    "            self.assigned_colors[key] = color\n",
    "            if not(self.colors): self.colors = self._original_colors.copy()\n",
    "            return color\n",
    "    \n",
    "    def assign(self, thing, color):\n",
    "        \"\"\"Assigns a specific color to a thing\"\"\"\n",
    "        key = id(thing)\n",
    "        self.assigned_colors[key] = color\n",
    "        if color in self.colors: self.colors.remove(color)\n",
    "\n",
    "\n",
    "def get_plotly_clusterspace(event, \n",
    "                            cluster_space_coords, \n",
    "                            clustering=None,\n",
    "                            size = 1.00):\n",
    "    assert cluster_space_coords.size(1) == 2\n",
    "    import plotly.graph_objects as go\n",
    "\n",
    "    colorwheel = ColorWheel()\n",
    "    colorwheel.assign(0, '#bfbfbf')\n",
    "    colorwheel.assign(-1, '#bfbfbf')\n",
    "\n",
    "    data = []\n",
    "\n",
    "    if clustering is None: clustering = event.y\n",
    "\n",
    "    for cluster_index in np.unique(clustering):\n",
    "        x = cluster_space_coords[clustering == cluster_index].numpy()\n",
    "        data.append(go.Scatter(\n",
    "            x=x[:,0], y=x[:,1],# z=x[:,2],\n",
    "            mode='markers', \n",
    "            marker=dict(\n",
    "                line=dict(width=0),\n",
    "                size=size,\n",
    "                color= colorwheel(int(cluster_index)),\n",
    "                ),\n",
    "            hovertemplate=(\n",
    "                f'x=%{{y:0.2f}}<br>y=%{{z:0.2f}}<br>z=%{{x:0.2f}}'\n",
    "                f'<br>clusterindex={cluster_index}'\n",
    "                f'<br>'\n",
    "                ),\n",
    "            name = f'cluster_{cluster_index}'\n",
    "            ))\n",
    "    return data\n",
    "\n",
    "def get_plotly_truth(event,\n",
    "                    size = 1.00):\n",
    "    colorwheel = ColorWheel()\n",
    "    colorwheel.assign(0, '#bfbfbf')\n",
    "\n",
    "    data = []\n",
    "\n",
    "    for cluster_index in np.unique(event.y):\n",
    "        x = event.x[event.y == cluster_index].numpy()\n",
    "        data.append(go.Scatter3d(\n",
    "            x=x[:,0], y=x[:,1], z=x[:,2],\n",
    "#             x=x[:,3],y=x[:,-1],z=x[:,2],\n",
    "            mode='lines+markers', \n",
    "            marker=dict(\n",
    "                line=dict(width=0),\n",
    "                size=size,\n",
    "                color= colorwheel(int(cluster_index)),\n",
    "                ),\n",
    "            hovertemplate=(\n",
    "                f'x=%{{y:0.2f}}<br>y=%{{z:0.2f}}<br>z=%{{x:0.2f}}'\n",
    "                f'<br>clusterindex={cluster_index}'\n",
    "                f'<br>'\n",
    "                ),\n",
    "            name = f'cluster_{cluster_index}'\n",
    "            ))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f9c2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "import glob\n",
    "\n",
    "import multiprocessing as mp\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import torch\n",
    "import pandas\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.utils import is_undirected\n",
    "from torch_geometric.data import Data, Dataset\n",
    "import gzip\n",
    "import pdb\n",
    "\n",
    "class TrackMLParticleTrackingDataset(Dataset):\n",
    "    def __init__(self, root, \n",
    "                 transform=None, \n",
    "                 n_events=0,\n",
    "                 directed=False, \n",
    "                 layer_pairs_plus=False,\n",
    "                 volume_layer_ids=[[8, 2], [8, 4], [8, 6], [8, 8]], #Layers Selecte\n",
    "                 layer_pairs=[[7, 8], [8, 9], [9, 10]],             #Connected Layers\n",
    "                 pt_min=0.3, \n",
    "                 eta_range=[-5, 5],                     \n",
    "                 phi_slope_max=0.0006, \n",
    "                 z0_max=150,                  \n",
    "                 n_phi_sections=1, \n",
    "                 n_eta_sections=1,  \n",
    "                 augments = False,\n",
    "                 tracking=False,                   \n",
    "                 n_workers=mp.cpu_count(), \n",
    "                 n_tasks=1,               \n",
    "                 download_full_dataset=False                        \n",
    "             ):\n",
    "        hits = glob.glob(osp.join(osp.join(root,'raw'), 'event*-hits.csv'))\n",
    "        self.hits = sorted(hits)\n",
    "        particles = glob.glob(osp.join(osp.join(root,'raw'), 'event*-particles.csv'))\n",
    "        self.particles = sorted(particles)\n",
    "        truth = glob.glob(osp.join(osp.join(root,'raw'), 'event*-truth.csv'))\n",
    "        self.truth = sorted(truth)\n",
    "        if (n_events > 0):\n",
    "            self.hits = self.hits[:n_events]\n",
    "            self.particles = self.particles[:n_events]\n",
    "            self.truth = self.truth[:n_events]\n",
    "        self.layer_pairs_plus = layer_pairs_plus\n",
    "        self.volume_layer_ids = torch.tensor(volume_layer_ids)\n",
    "        self.layer_pairs      = torch.tensor(layer_pairs)\n",
    "        self.pt_min           = pt_min\n",
    "        self.eta_range        = eta_range\n",
    "        self.n_phi_sections   = n_phi_sections\n",
    "        self.n_eta_sections   = n_eta_sections\n",
    "        self.full_dataset     = download_full_dataset\n",
    "        self.n_events         = n_events\n",
    "\n",
    "#         self.phi_slope_max    = phi_slope_max\n",
    "#         self.z0_max           = z0_max\n",
    "#         self.augments         = augments\n",
    "#         self.tracking         = tracking\n",
    "#         self.n_tasks          = n_tasks\n",
    "\n",
    "        super(TrackMLParticleTrackingDataset, self).__init__(root, transform)\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        #if not hasattr(self,'input_files'):\n",
    "        self.input_files = sorted(glob.glob(self.raw_dir+'/*.csv'))\n",
    "        return [f.split('/')[-1] for f in self.input_files]\n",
    "\n",
    "    def len(self):\n",
    "        N_events = len(self.hits)\n",
    "        return N_events*self.n_phi_sections*self.n_eta_sections\n",
    "\n",
    "    def __len__(self):\n",
    "        N_events = len(self.hits)\n",
    "        return N_events*self.n_phi_sections*self.n_eta_sections\n",
    "\n",
    "    def read_events(self,idx):\n",
    "        hits_filename = self.hits[idx]\n",
    "        hits = pandas.read_csv(\n",
    "            hits_filename, usecols=['hit_id', 'x', 'y', 'z', 'volume_id', 'layer_id', 'module_id'],\n",
    "            dtype={\n",
    "                'hit_id': np.int64,\n",
    "                'x': np.float32,\n",
    "                'y': np.float32,\n",
    "                'z': np.float32,\n",
    "                'volume_id': np.int64,\n",
    "                'layer_id': np.int64,\n",
    "                'module_id': np.int64\n",
    "            })\n",
    "        particles_filename = self.particles[idx]\n",
    "        particles = pandas.read_csv(\n",
    "            particles_filename, usecols=['particle_id', 'vx', 'vy', 'vz', 'px', 'py', 'pz', 'q', 'nhits'],\n",
    "            dtype={\n",
    "                'particle_id': np.int64,\n",
    "                'vx': np.float32,\n",
    "                'vy': np.float32,\n",
    "                'vz': np.float32,\n",
    "                'px': np.float32,\n",
    "                'py': np.float32,\n",
    "                'pz': np.float32,\n",
    "                'q': np.int64,\n",
    "                'nhits': np.int64\n",
    "            })\n",
    "        truth_filename = self.truth[idx]\n",
    "        truth = pandas.read_csv(\n",
    "            truth_filename, usecols=['hit_id', 'particle_id', 'tx', 'ty', 'tz', 'tpx', 'tpy', 'tpz', 'weight'],\n",
    "            dtype={\n",
    "                'hit_id': np.int64,\n",
    "                'particle_id': np.int64,\n",
    "                'tx': np.float32,\n",
    "                'ty': np.float32,\n",
    "                'tz': np.float32,\n",
    "                'tpx': np.float32,\n",
    "                'tpy': np.float32,\n",
    "                'tpz': np.float32,\n",
    "                'weight': np.float32\n",
    "            })\n",
    "        return hits,particles,truth\n",
    "\n",
    "\n",
    "    def download(self):\n",
    "        import os        \n",
    "        from zipfile import ZipFile\n",
    "        try:\n",
    "            from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "        except ImportError:\n",
    "            raise RuntimeError('please install and setup the kaggle '\n",
    "                               'competition api: https://github.com/Kaggle/kaggle-api')\n",
    "        \n",
    "        api = KaggleApi()\n",
    "        api.authenticate()\n",
    "        \n",
    "        kgl_comp = 'trackml-particle-identification'\n",
    "        test_file = 'train_sample.zip'\n",
    "\n",
    "        if self.full_dataset:\n",
    "            kgl_file = 'trackml-particle-identification.zip'\n",
    "            print('Downloading full TrackML dataset (~80GB), this may take a while...')\n",
    "            api.competition_download_files(kgl_comp, \n",
    "                                           path=self.root,\n",
    "                                           quiet = False,\n",
    "                                           force = False)\n",
    "            training_samples = None\n",
    "            with ZipFile(os.path.join(self.root,kgl_file), 'r') as zf:\n",
    "                training_samples = [fname for fname in filter(lambda x: 'train' in x and \\\n",
    "                                                                        'sample' not in x and \\\n",
    "                                                                        'blacklist' not in x, \n",
    "                                                              zf.namelist())]\n",
    "                \n",
    "                for name in tqdm(training_samples, desc='extracting zipballs'):\n",
    "                    if not os.path.exists(os.path.join(self.root, name)):\n",
    "                        zf.extract(name, path=self.root)\n",
    "                        \n",
    "            for sample in training_samples:\n",
    "                with ZipFile(os.path.join(self.root,sample), 'r') as zf:\n",
    "                    fnames = zf.namelist()\n",
    "                    action = f'unpacking {sample}'\n",
    "                    for name in tqdm(fnames, desc=action):\n",
    "                        sample_dir = sample.split('.')[0] + '/'\n",
    "                        if name == sample_dir:\n",
    "                            continue\n",
    "                        outname = os.path.join(self.raw_dir, os.path.basename(name))\n",
    "                        if os.path.exists(outname):\n",
    "                            raise Exception(f'{outname} already exists!')\n",
    "                        with open(outname, 'wb') as fout:\n",
    "                            fout.write(zf.read(name))\n",
    "                                          \n",
    "        else:\n",
    "            kgl_file = test_file\n",
    "            print('Downloading training example from TrackML dataset, only 100 training events...')\n",
    "            api.competition_download_file(kgl_comp, \n",
    "                                          test_file,\n",
    "                                          path=self.root,\n",
    "                                          quiet = False,\n",
    "                                          force = False)\n",
    "            with ZipFile(os.path.join(self.root,kgl_file), 'r') as zf:\n",
    "                fnames = zf.namelist()\n",
    "                for name in tqdm(fnames):\n",
    "                    if name == 'train_100_events/': \n",
    "                        continue\n",
    "                    with open(os.path.join(self.raw_dir, os.path.basename(name)), 'wb') as fout:\n",
    "                        fout.write(zf.read(name))\n",
    "\n",
    "        events = glob.glob(osp.join(osp.join(self.root, 'raw'), 'event*-hits.csv'))\n",
    "        events = [e.split(osp.sep)[-1].split('-')[0][5:] for e in events]\n",
    "        self.events = sorted(events)\n",
    "        if (self.n_events > 0):\n",
    "            self.events = self.events[:self.n_events]\n",
    "\n",
    "\n",
    "    def select_all_hits(self,hits,particles,truth, noise_label = -1):\n",
    "        hits_truth = hits.merge(truth[[\"hit_id\", \"particle_id\"]], on=\"hit_id\", how=\"left\")\n",
    "\n",
    "        full_truth = hits_truth.merge(particles[[\"px\",\"py\",\"pz\",\"particle_id\"]], on=\"particle_id\", how=\"left\")\n",
    "        full_truth[[\"px\", \"py\", \"pz\"]] = full_truth[[\"px\", \"py\", \"pz\"]].fillna(0)\n",
    "\n",
    "        full_truth[\"pt\"] = np.sqrt(full_truth[\"px\"]**2 + full_truth[\"py\"]**2)\n",
    "        full_truth[\"r\"] = np.sqrt(full_truth[\"x\"].values**2 + full_truth[\"y\"].values**2)\n",
    "        full_truth[\"phi\"] = np.arctan2(full_truth[\"y\"].values, full_truth[\"x\"].values)\n",
    "        full_truth[\"theta\"] = np.arctan2(full_truth[\"r\"].values,full_truth[\"z\"].values)\n",
    "        full_truth[\"eta\"] = -1*np.log(np.tan(full_truth[\"theta\"]/2.))\n",
    "\n",
    "        pids_unique, pids_inverse, pids_counts = np.unique(full_truth['particle_id'].values, return_inverse=True, return_counts=True)\n",
    "        pids_unique = np.arange(pids_unique.size) \n",
    "        full_truth[\"remapped_pid\"] = pids_unique[pids_inverse]\n",
    "        # here we have the reconstructed number of hits for each track\n",
    "        full_truth[\"nhits\"] = pids_counts[pids_inverse]\n",
    "\n",
    "        # now we have full information to label tracks and their hits as noise based on various properties\n",
    "        full_truth['remapped_pid'].where((full_truth['nhits'] >= 2) & (full_truth['pt'] > self.pt_min), 0, inplace=True)\n",
    "\n",
    "        # re-calculate counts and such\n",
    "        pids_unique, pids_inverse, pids_counts = np.unique(full_truth['remapped_pid'].values, return_inverse=True, return_counts=True)\n",
    "        pids_unique = np.arange(pids_unique.size) \n",
    "        full_truth[\"remapped_pid\"] = pids_unique[pids_inverse]\n",
    "        # here we have the reconstructed number of hits for each track\n",
    "        full_truth[\"nhits\"] = pids_counts[pids_inverse]\n",
    "\n",
    "        # Select a subset noise + tracks to be selected as signals\n",
    "        noise = full_truth[full_truth.remapped_pid == 0]\n",
    "        idx = random.sample(noise.index.to_list(),100)\n",
    "        idx.sort()\n",
    "        noise = noise.loc[idx]\n",
    "        \n",
    "        signal_tracks = [i for i in range(1, 51)]\n",
    "        signals = full_truth[full_truth.remapped_pid.isin(signal_tracks)]\n",
    "        \n",
    "        selected_hits = pd.concat([signals,noise])\n",
    "\n",
    "        # Extract the features from the selected_hits\n",
    "        x = torch.from_numpy(selected_hits['x'].values)\n",
    "        y = torch.from_numpy(selected_hits['y'].values)\n",
    "        theta = torch.from_numpy(selected_hits['theta'].values)\n",
    "        r = torch.from_numpy(selected_hits['r'].values)\n",
    "        phi = torch.from_numpy(selected_hits['phi'].values)\n",
    "        z = torch.from_numpy(selected_hits['z'].values)\n",
    "        eta = torch.from_numpy(selected_hits['eta'].values)\n",
    "        particle_labels = torch.from_numpy(selected_hits['remapped_pid'].values)\n",
    "        pos = torch.stack([x, y, z, r, theta, phi], 1)   \n",
    "\n",
    "        #print(\"selected hits\")\n",
    "        #print(selected_hits)\n",
    "        #print(\"selected hits\")\n",
    "        \n",
    "        return pos, eta, particle_labels\n",
    "    \n",
    "    def split_detector_sections(self,pos, eta, particle_labels, phi_edges, eta_edges):\n",
    "        pos_sect, particle_label_sect = [], []\n",
    "        # Refer to the index of the column representing phi values in pos tensor\n",
    "        phi_idx = -1\n",
    "        for i in range(len(phi_edges) - 1):\n",
    "            phi_mask1 = pos[:,phi_idx] > phi_edges[i]\n",
    "            phi_mask2 = pos[:,phi_idx] < phi_edges[i+1]\n",
    "            phi_mask  = phi_mask1 & phi_mask2\n",
    "            phi_pos      = pos[phi_mask]\n",
    "            phi_eta      = eta[phi_mask]\n",
    "            phi_particle_label = particle_labels[phi_mask]\n",
    "\n",
    "            for j in range(len(eta_edges) - 1):\n",
    "                eta_mask1 = phi_eta > eta_edges[j]\n",
    "                eta_mask2 = phi_eta < eta_edges[j+1]\n",
    "                eta_mask  = eta_mask1 & eta_mask2\n",
    "                phi_eta_pos = phi_pos[eta_mask]\n",
    "                phi_eta_particle_label = phi_particle_label[eta_mask]\n",
    "                pos_sect.append(phi_eta_pos)\n",
    "                particle_label_sect.append(phi_eta_particle_label)\n",
    "\n",
    "        return pos_sect, particle_label_sect\n",
    "    \n",
    "    def get(self,idx):\n",
    "        \n",
    "        hits,particles,truth = self.read_events(idx)   \n",
    "        pos, eta, particle_labels = self.select_all_hits(hits, \n",
    "                                                         particles, \n",
    "                                                         truth,noise_label=0)\n",
    "        tracks = torch.empty(0, 5, dtype=torch.long)  \n",
    "        phi_edges = np.linspace(*(-np.pi, np.pi), num=self.n_phi_sections+1)\n",
    "        eta_edges = np.linspace(*self.eta_range, num=self.n_eta_sections+1)\n",
    "        pos_sect, particle_label_sect = self.split_detector_sections(pos, \n",
    "                                                                    eta,\n",
    "                                                                    particle_labels, \n",
    "                                                                    phi_edges, \n",
    "                                                                    eta_edges)\n",
    "        for i in range(len(pos_sect)):\n",
    "            y = particle_label_sect[0]\n",
    "            return Data(x=pos_sect[0],\n",
    "                        y=y,\n",
    "                        tracks=tracks,\n",
    "                        inpz = torch.Tensor([i]))\n",
    "\n",
    "def fetch_dataloader(data_dir, \n",
    "                     batch_size, \n",
    "                     validation_split,\n",
    "                     n_events = 100,\n",
    "                     pt_min = 0.3,\n",
    "                     n_workers = 1,\n",
    "                     generate_tracks = True,\n",
    "                     full_dataset = False,\n",
    "                     shuffle=False):\n",
    "    volume_layer_ids = [\n",
    "        [8, 2], [8, 4], [8, 6], [8, 8], # barrel pixels\n",
    "        [7, 2], [7, 4], [7, 6], [7, 8], [7, 10], [7, 12], [7, 14],# minus pixel endcap\n",
    "        [9, 2], [9, 4], [9, 6], [9, 8], [9, 10], [9, 12], [9, 14], # plus pixel endcap\n",
    "    ]\n",
    "    dataset = TrackMLParticleTrackingDataset(root=data_dir,\n",
    "                                             layer_pairs_plus=True, \n",
    "                                             pt_min= pt_min,\n",
    "                                             volume_layer_ids=volume_layer_ids,\n",
    "                                             n_events=n_events, \n",
    "                                             n_workers=n_workers, \n",
    "                                             tracking = generate_tracks,\n",
    "                                             download_full_dataset=full_dataset)\n",
    "    dataset_size = len(dataset)\n",
    "    indices = list(range(dataset_size))\n",
    "    if dataset_size > 2:\n",
    "        split = int(np.floor(validation_split * dataset_size))\n",
    "    else: \n",
    "        split = 1\n",
    "    print(split)\n",
    "    random_seed= 1001\n",
    "\n",
    "    train_subset, val_subset = torch.utils.data.random_split(dataset, [dataset_size - split, split],\n",
    "                                                             generator=torch.Generator().manual_seed(random_seed))\n",
    "    print(\"train subset dim:\", len(train_subset))\n",
    "    print(\"validation subset dim\", len(val_subset))\n",
    "    dataloaders = {\n",
    "        'train':  DataLoader(train_subset, batch_size=batch_size, shuffle=shuffle),\n",
    "        'val':   DataLoader(val_subset, batch_size=1, shuffle=shuffle)\n",
    "        }\n",
    "    print(\"train_dataloader dim:\", len(dataloaders['train']))\n",
    "    print(\"val dataloader dim:\", len(dataloaders['val']))\n",
    "    return dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98efb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_offset: float = 0.0\n",
    "\n",
    "def compute_oc_loss(out, data, s_c=1., return_components=True):\n",
    "    device = out.device\n",
    "    pred_betas = torch.sigmoid(out[:,0])\n",
    "    #print(pred_betas)\n",
    "    pred_cluster_space_coords = out[:,1:]\n",
    "    assert all(t.device == device for t in [\n",
    "        pred_betas, pred_cluster_space_coords, data.y,\n",
    "        data.batch,\n",
    "        ])\n",
    "           \n",
    "    out_oc = calc_LV_Lbeta(\n",
    "        pred_betas,\n",
    "        pred_cluster_space_coords,\n",
    "        data.y.long(),\n",
    "        data.batch,\n",
    "        return_components=return_components,\n",
    "        qmin = 0.1,\n",
    "        )\n",
    "    \n",
    "    # print(out_oc)\n",
    "    print(formatted_loss_components_string(out_oc))\n",
    "    \n",
    "    #print(out_oc[\"L_V\"])\n",
    "    #print(out_oc[\"L_beta\"])\n",
    "    \n",
    "    LV = out_oc[\"L_V\"]\n",
    "    Lbeta = out_oc[\"L_beta\"]\n",
    "    return LV + Lbeta + loss_offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc35b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data_loader, model, epoch, optimizer,interval = 1):\n",
    "    print('Training epoch', epoch)\n",
    "    model.train()\n",
    "    data = tqdm(data_loader, total=len(data_loader))\n",
    "    data.set_postfix({'loss': '?'})\n",
    "    for i,inputs in enumerate(data):\n",
    "        inputs.to('cuda')\n",
    "        #print(\"inputs:\", inputs.x)\n",
    "        optimizer.zero_grad()\n",
    "        result = model(inputs.x, inputs.batch)\n",
    "        #print(result)\n",
    "        loss = compute_oc_loss(result,inputs)\n",
    "        #if i % interval == 0:\n",
    "        #    print(f'loss={float(loss)}')\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        data.set_postfix({'loss': float(loss)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8159f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(data_loader, model, epoch, generate_plots = False):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        loss = 0.\n",
    "        data = tqdm(data_loader, total=len(data_loader))\n",
    "        for i,inputs in enumerate(data):\n",
    "            inputs.to('cuda')\n",
    "            result = model(inputs.x, inputs.batch)\n",
    "            loss +=  compute_oc_loss(result,inputs)\n",
    "            pred_betas = torch.sigmoid(result[:,0])\n",
    "            pred_cluster_space_coords = result[:,1:]\n",
    "            if generate_plots:\n",
    "                fig = go.Figure(get_plotly_truth(inputs.to('cpu'),size = 2.75))\n",
    "                fig.write_html(\"plots/truth_plot_epoch_\"+str(epoch+1)+\"_batch_\"+str(i+1)+\".html\")\n",
    "                pred_fig=go.Figure(get_plotly_clusterspace(inputs.to('cpu'),pred_cluster_space_coords.to('cpu'),size = 2.75))\n",
    "                pred_fig.write_html(\"plots/predictions_plot_epoch_\"+str(epoch+1)+\"_batch_\"+str(i+1)+\".html\")\n",
    "        loss /= len(data_loader)\n",
    "        print(f'Avg test loss: {loss} {len(data_loader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f84894",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Root path settings work ONLY WHEN training files are in the same directory as the code\n",
    "root = osp.join(\"/mnt/c/Users/linds/trackml/train_1\")\n",
    "noise_pt_min = 0.3\n",
    "batch_size = 10\n",
    "validation_split = 0.1\n",
    "events = 200\n",
    "model_input_dim = 6\n",
    "model_output_dim = 3\n",
    "data = fetch_dataloader(data_dir = root,\n",
    "                        batch_size = batch_size,\n",
    "                        validation_split=validation_split,\n",
    "                        full_dataset = True,\n",
    "                        n_events = events,\n",
    "                        pt_min = noise_pt_min,\n",
    "                        shuffle=True)\n",
    "epochs = 100\n",
    "train_loader,test_loader = data['train'],data['val']\n",
    "model = GravnetModel(input_dim=model_input_dim,output_dim=model_output_dim).to('cuda')\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-3)\n",
    "\n",
    "for i_epoch in range(epochs):\n",
    "    train(train_loader, \n",
    "          model, \n",
    "          i_epoch, \n",
    "          optimizer,\n",
    "          interval = 1)\n",
    "    test(test_loader, \n",
    "        model, \n",
    "        i_epoch,\n",
    "        generate_plots = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5207d851",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### END ############################################################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
