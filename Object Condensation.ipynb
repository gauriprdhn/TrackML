{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05de48b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"essential packages\"\n",
    "import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "from torch_scatter import scatter, scatter_max, scatter_add\n",
    "import glob\n",
    "import multiprocessing as mp\n",
    "import random\n",
    "import pandas\n",
    "from torch_geometric.utils import is_undirected\n",
    "from torch_geometric.data import Data, Dataset, DataLoader\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "from torch.nn import Linear, ReLU\n",
    "# from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.nn.conv.gravnet_conv import GravNetConv\n",
    "\"libraries for debugging\"\n",
    "import sys\n",
    "import os.path as osp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16cee65",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"custom imports\"\n",
    "from dataloader import fetch_dataloader\n",
    "from graphconv import GravnetModel\n",
    "from objectcondensation import calc_Lp, calc_LV_Lbeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b628c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TrackMLParticleTrackingDataset(Dataset):\n",
    "#     def __init__(self, root, \n",
    "#                  transform=None, \n",
    "#                  n_events=0,\n",
    "#                  directed=False, \n",
    "#                  layer_pairs_plus=False,\n",
    "#                  volume_layer_ids=[[8, 2], [8, 4], [8, 6], [8, 8]], #Layers Selecte\n",
    "#                  layer_pairs=[[7, 8], [8, 9], [9, 10]],             #Connected Layers\n",
    "#                  pt_min=2.0, \n",
    "#                  eta_range=[-5, 5],                     \n",
    "#                  phi_slope_max=0.0006, \n",
    "#                  z0_max=150,                  \n",
    "#                  n_phi_sections=1, \n",
    "#                  n_eta_sections=1,                \n",
    "#                  augments=False, \n",
    "#                  intersect=False, \n",
    "#                  tracking=False,                   \n",
    "#                  n_workers=mp.cpu_count(), \n",
    "#                  n_tasks=1,               \n",
    "#                  download_full_dataset=False                        \n",
    "#              ):\n",
    "#         hits = glob.glob(osp.join(osp.join(root,'raw'), 'event*-hits.csv'))\n",
    "#         self.hits = sorted(hits)\n",
    "#         particles = glob.glob(osp.join(osp.join(root,'raw'), 'event*-particles.csv'))\n",
    "#         self.particles = sorted(particles)\n",
    "#         truth = glob.glob(osp.join(osp.join(root,'raw'), 'event*-truth.csv'))\n",
    "#         self.truth = sorted(truth)\n",
    "#         if (n_events > 0):\n",
    "#             self.hits = self.hits[:n_events]\n",
    "#             self.particles = self.particles[:n_events]\n",
    "#             self.truth = self.truth[:n_events]\n",
    "#         self.directed         = directed\n",
    "#         self.layer_pairs_plus = layer_pairs_plus\n",
    "#         self.volume_layer_ids = torch.tensor(volume_layer_ids)\n",
    "#         self.layer_pairs      = torch.tensor(layer_pairs)\n",
    "#         self.pt_min           = pt_min\n",
    "#         self.eta_range        = eta_range\n",
    "#         self.phi_slope_max    = phi_slope_max\n",
    "#         self.z0_max           = z0_max\n",
    "#         self.n_phi_sections   = n_phi_sections\n",
    "#         self.n_eta_sections   = n_eta_sections\n",
    "#         self.augments         = augments\n",
    "#         self.intersect        = intersect\n",
    "#         self.tracking         = tracking\n",
    "#         self.n_workers        = n_workers\n",
    "#         self.n_tasks          = n_tasks\n",
    "#         self.full_dataset     = download_full_dataset\n",
    "#         self.n_events         = n_events\n",
    "\n",
    "#         super(TrackMLParticleTrackingDataset, self).__init__(root, transform)\n",
    "\n",
    "#     def len(self):\n",
    "#         N_events = len(self.hits)\n",
    "#         N_augments = 2 if self.augments else 1\n",
    "#         return N_events*self.n_phi_sections*self.n_eta_sections*N_augments\n",
    "\n",
    "#     def __len__(self):\n",
    "#         N_events = len(self.hits)\n",
    "#         N_augments = 2 if self.augments else 1\n",
    "#         return N_events*self.n_phi_sections*self.n_eta_sections*N_augments\n",
    "\n",
    "#     def read_events(self,idx):\n",
    "#         hits_filename = self.hits[idx]\n",
    "#         hits = pandas.read_csv(\n",
    "#             hits_filename, usecols=['hit_id', 'x', 'y', 'z', 'volume_id', 'layer_id', 'module_id'],\n",
    "#             dtype={\n",
    "#                 'hit_id': np.int64,\n",
    "#                 'x': np.float32,\n",
    "#                 'y': np.float32,\n",
    "#                 'z': np.float32,\n",
    "#                 'volume_id': np.int64,\n",
    "#                 'layer_id': np.int64,\n",
    "#                 'module_id': np.int64\n",
    "#             })\n",
    "#         particles_filename = self.particles[idx]\n",
    "#         particles = pandas.read_csv(\n",
    "#             particles_filename, usecols=['particle_id', 'vx', 'vy', 'vz', 'px', 'py', 'pz', 'q', 'nhits'],\n",
    "#             dtype={\n",
    "#                 'particle_id': np.int64,\n",
    "#                 'vx': np.float32,\n",
    "#                 'vy': np.float32,\n",
    "#                 'vz': np.float32,\n",
    "#                 'px': np.float32,\n",
    "#                 'py': np.float32,\n",
    "#                 'pz': np.float32,\n",
    "#                 'q': np.int64,\n",
    "#                 'nhits': np.int64\n",
    "#             })\n",
    "#         truth_filename = self.truth[idx]\n",
    "#         truth = pandas.read_csv(\n",
    "#             truth_filename, usecols=['hit_id', 'particle_id', 'tx', 'ty', 'tz', 'tpx', 'tpy', 'tpz', 'weight'],\n",
    "#             dtype={\n",
    "#                 'hit_id': np.int64,\n",
    "#                 'particle_id': np.int64,\n",
    "#                 'tx': np.float32,\n",
    "#                 'ty': np.float32,\n",
    "#                 'tz': np.float32,\n",
    "#                 'tpx': np.float32,\n",
    "#                 'tpy': np.float32,\n",
    "#                 'tpz': np.float32,\n",
    "#                 'weight': np.float32\n",
    "#             })\n",
    "#         return hits,particles,truth\n",
    "    \n",
    "#     def select_hits(self, hits, particles, truth):\n",
    "#         # print('Selecting Hits')\n",
    "#         valid_layer = 20 * self.volume_layer_ids[:,0] + self.volume_layer_ids[:,1]\n",
    "#         n_det_layers = len(valid_layer)\n",
    "\n",
    "#         layer = torch.from_numpy(20 * hits['volume_id'].values + hits['layer_id'].values)\n",
    "#         index = layer.unique(return_inverse=True)[1]\n",
    "#         hits = hits[['hit_id', 'x', 'y', 'z']].assign(layer=layer, index=index)\n",
    "\n",
    "#         particles['pt'] = np.sqrt(particles['px']**2 + particles['py']**2)\n",
    "#         particles['pmag'] = np.sqrt(particles['pt']**2 + particles['pz']**2)\n",
    "#         particles['eta'] = 0.5*(np.log(particles['pmag'] + particles['pz']) - np.log(particles['pmag'] - particles['pz']))\n",
    "#         particles['phi'] = np.arctan2(particles['py'], particles['px'])\n",
    "        \n",
    "#         pt = np.sqrt(particles['px'].values**2 + particles['py'].values**2)\n",
    "#         particles_mask = pt > self.pt_min\n",
    "#         particles_fail = particles[~particles_mask]\n",
    "#         particles = particles[particles_mask]\n",
    "\n",
    "#         valid_groups = hits.groupby(['layer'])\n",
    "#         hits = pandas.concat([valid_groups.get_group(valid_layer.numpy()[i]) for i in range(n_det_layers)])\n",
    "\n",
    "#         hits = (hits[['hit_id', 'x', 'y', 'z', 'index']].merge(truth[['hit_id', 'particle_id']], on='hit_id'))\n",
    "#         hits['track_id'] = hits['hit_id'].astype(str) + \"-\" + hits['particle_id'].astype(str)\n",
    "\n",
    "#         hits['particle_id'].where(hits['particle_id'].isin(particles['particle_id']) | (hits['particle_id'] == 0), -1, inplace=True)\n",
    "#         pids_unique, pids_inverse, pids_counts = np.unique(hits['particle_id'].values, return_inverse=True, return_counts=True)        \n",
    "#         pids_unique = np.arange(pids_unique.size) # make it [not interested, noise, remapped pid]\n",
    "#         hits['remapped_pid'] = pids_unique[pids_inverse]\n",
    "\n",
    "#         hits = hits[(hits['remapped_pid'] > 0) & (hits['remapped_pid'] < (200 + hits.size%50))]\n",
    "#         hits['remapped_pid'] = hits['remapped_pid'] - 1\n",
    "#         r = np.sqrt(hits['x'].values**2 + hits['y'].values**2)\n",
    "#         phi = np.arctan2(hits['y'].values, hits['x'].values)\n",
    "#         theta = np.arctan2(r,hits['z'].values)\n",
    "#         eta = -1*np.log(np.tan(theta/2))\n",
    "#         hits = hits[['track_id','z', 'index', 'particle_id', 'remapped_pid']].assign(r=r, phi=phi, eta=eta)\n",
    "\n",
    "#         # Remove duplicate hits\n",
    "#         if not self.layer_pairs_plus:\n",
    "#             hits = hits.loc[hits.groupby(['particle_id', 'index'], as_index=False).r.idxmin()]\n",
    "#         particles = particles[['particle_id','q','pt','eta','phi']]\n",
    "#         if self.tracking:\n",
    "#             return hits,particles\n",
    "#         else:                    \n",
    "#             r = torch.from_numpy(hits['r'].values)\n",
    "#             phi = torch.from_numpy(hits['phi'].values)\n",
    "#             z = torch.from_numpy(hits['z'].values)\n",
    "#             eta = torch.from_numpy(hits['eta'].values)\n",
    "#             layer = torch.from_numpy(hits['index'].values)\n",
    "#             particle = torch.from_numpy(hits['particle_id'].values)\n",
    "#             plabel = torch.from_numpy(hits['remapped_pid'].values)\n",
    "#             pos = torch.stack([r, phi, z], 1)\n",
    "\n",
    "#             return  pos, layer, particle, eta, plabel, particles\n",
    "\n",
    "#     def split_detector_sections(self, pos, layer, particle, eta, particle_label, phi_edges, eta_edges):\n",
    "#         pos_sect, layer_sect, particle_sect, particle_label_sect = [], [], [], []\n",
    "\n",
    "#         for i in range(len(phi_edges) - 1):\n",
    "#             phi_mask1 = pos[:,1] > phi_edges[i]\n",
    "#             phi_mask2 = pos[:,1] < phi_edges[i+1]\n",
    "#             phi_mask  = phi_mask1 & phi_mask2\n",
    "#             phi_pos      = pos[phi_mask]\n",
    "#             phi_layer    = layer[phi_mask]\n",
    "#             phi_particle = particle[phi_mask]\n",
    "#             phi_eta      = eta[phi_mask]\n",
    "#             phi_particle_label = particle_label[phi_mask]\n",
    "\n",
    "#             for j in range(len(eta_edges) - 1):\n",
    "#                 eta_mask1 = phi_eta > eta_edges[j]\n",
    "#                 eta_mask2 = phi_eta < eta_edges[j+1]\n",
    "#                 eta_mask  = eta_mask1 & eta_mask2\n",
    "#                 phi_eta_pos = phi_pos[eta_mask]\n",
    "#                 phi_eta_layer = phi_layer[eta_mask]\n",
    "#                 phi_eta_particle = phi_particle[eta_mask]\n",
    "#                 phi_eta_particle_label = phi_particle_label[eta_mask]\n",
    "#                 pos_sect.append(phi_eta_pos)\n",
    "#                 layer_sect.append(phi_eta_layer)\n",
    "#                 particle_sect.append(phi_eta_particle)\n",
    "#                 particle_label_sect.append(phi_eta_particle_label)\n",
    "\n",
    "#         return pos_sect, layer_sect, particle_sect, particle_label_sect\n",
    "\n",
    "#     def build_tracks(self,selected_hits,hits, particles, truth):\n",
    "\n",
    "#         tensors = []\n",
    "\n",
    "#         valid_layer = 20 * self.volume_layer_ids[:,0] + self.volume_layer_ids[:,1]\n",
    "#         hits = (hits[['hit_id', 'x', 'y', 'z', 'volume_id', 'layer_id']]\n",
    "#                 .merge(truth[['hit_id', 'particle_id']], on='hit_id'))\n",
    "#         hits = (hits[['hit_id', 'x', 'y', 'z', 'volume_id', 'layer_id', 'particle_id']]\n",
    "#                 .merge(particles[['particle_id', 'px', 'py', 'pz']], on='particle_id'))\n",
    "#         hits['track_id'] = hits['hit_id'].astype(str) + \"-\" + hits['particle_id'].astype(str)\n",
    "#         hits = hits.merge(selected_hits, on = 'track_id') \n",
    "\n",
    "#         r = torch.from_numpy(hits['r'].values)\n",
    "#         phi = torch.from_numpy(hits['phi'].values)\n",
    "#         z = torch.from_numpy(hits['z_x'].values)\n",
    "#         eta = torch.from_numpy(hits['eta'].values)\n",
    "#         layer = torch.from_numpy(hits['index'].values)\n",
    "#         particle = torch.from_numpy(hits['particle_id_x'].values)\n",
    "#         plabel = torch.from_numpy(hits['remapped_pid'].values)\n",
    "#         pos = torch.stack([r, phi, z], 1)   \n",
    "#         tensors.extend([pos, layer, particle, eta, plabel])\n",
    "\n",
    "#         layer = torch.from_numpy(20 * hits['volume_id'].values + hits['layer_id'].values)\n",
    "#         r = torch.from_numpy(np.sqrt(hits['x'].values**2 + hits['y'].values**2))\n",
    "#         phi = torch.from_numpy(np.arctan2(hits['y'].values, hits['x'].values))\n",
    "#         z = torch.from_numpy(hits['z_x'].values)\n",
    "#         pt = torch.from_numpy(np.sqrt(hits['px'].values**2 + hits['py'].values**2))\n",
    "#         particle = torch.from_numpy(hits['particle_id_x'].values)\n",
    "#         layer_mask = torch.from_numpy(np.isin(layer, valid_layer))\n",
    "#         pt_mask = pt > self.pt_min\n",
    "#         mask = pt_mask\n",
    "\n",
    "#         layer = layer.unique(return_inverse=True)[1]\n",
    "#         r = r[mask]\n",
    "#         phi = phi[mask]\n",
    "#         z = z[mask]\n",
    "#         pos = torch.stack([r, phi, z], 1)\n",
    "#         particle = particle[mask]\n",
    "#         layer = layer[mask]\n",
    "#         particle, indices = torch.sort(particle)\n",
    "#         particle = particle.unique(return_inverse=True)[1]\n",
    "#         pos = pos[indices]\n",
    "#         layer = layer[indices]\n",
    "#         tracks = torch.empty(0,5, dtype=torch.float)\n",
    "\n",
    "#         for i in range(particle.max()+1):\n",
    "#             track_pos   = pos[particle == i]\n",
    "#             track_layer = layer[particle == i]\n",
    "#             track_particle = particle[particle == i]\n",
    "#             track_layer, indices = torch.sort(track_layer)\n",
    "#             track_pos = track_pos[indices]\n",
    "#             track_layer = track_layer[:, None]\n",
    "#             track_particle = track_particle[:, None]\n",
    "#             track = torch.cat((track_pos, track_layer.type(torch.float)), 1)\n",
    "#             track = torch.cat((track, track_particle.type(torch.float)), 1)\n",
    "#             tracks = torch.cat((tracks, track), 0)  \n",
    "#         tensors.append(tracks)\n",
    "#         return tensors\n",
    "\n",
    "\n",
    "#     def compute_edge_index(self, pos, layer):\n",
    "#         # print(\"Constructing Edge Index\")\n",
    "#         edge_indices = torch.empty(2,0, dtype=torch.long)\n",
    "\n",
    "#         layer_pairs = self.layer_pairs\n",
    "#         if self.layer_pairs_plus:\n",
    "#             layers = layer.unique()\n",
    "#             layer_pairs_plus = torch.tensor([[layers[i],layers[i]] for i in range(layers.shape[0])])\n",
    "#             layer_pairs = torch.cat((layer_pairs, layer_pairs_plus), 0)\n",
    "\n",
    "#         for (layer1, layer2) in layer_pairs:\n",
    "#             mask1 = layer == layer1\n",
    "#             mask2 = layer == layer2\n",
    "#             nnz1 = mask1.nonzero().flatten()\n",
    "#             nnz2 = mask2.nonzero().flatten()\n",
    "\n",
    "#             dr   = pos[:, 0][mask2].view(1, -1) - pos[:, 0][mask1].view(-1, 1)\n",
    "#             dphi = pos[:, 1][mask2].view(1, -1) - pos[:, 1][mask1].view(-1, 1)\n",
    "#             dz   = pos[:, 2][mask2].view(1, -1) - pos[:, 2][mask1].view(-1, 1)\n",
    "#             dphi[dphi > np.pi] -= 2 * np.pi\n",
    "#             dphi[dphi < -np.pi] += 2 * np.pi\n",
    "\n",
    "#             # Calculate phi_slope and z0 which will be cut on\n",
    "#             phi_slope = dphi / dr\n",
    "#             z0 = pos[:, 2][mask1].view(-1, 1) - pos[:, 0][mask1].view(-1, 1) * dz / dr\n",
    "#             # Calculate phi_slope and z0 which will be cut on\n",
    "#             phi_slope = dphi / dr\n",
    "#             z0 = pos[:, 2][mask1].view(-1, 1) - pos[:, 0][mask1].view(-1, 1) * dz / dr\n",
    "\n",
    "#             # Check for intersecting edges between barrel and endcap connections\n",
    "#             intersected_layer = dr.abs() < -1\n",
    "#             if (self.intersect):\n",
    "#                 if((layer1 == 7 and (layer2 == 6 or layer2 == 11)) or\n",
    "#                    (layer2 == 7 and (layer1 == 6 or layer1 == 11))):\n",
    "#                     z_int =  71.56298065185547 * dz / dr + z0\n",
    "#                     intersected_layer = z_int.abs() < 490.975\n",
    "#                 elif((layer1 == 8 and (layer2 == 6 or layer2 == 11)) or\n",
    "#                      (layer2 == 8 and (layer1 == 6 or layer1 == 11))):\n",
    "#                     z_int = 115.37811279296875 * dz / dr + z0\n",
    "#                     intersected_layer = z_int.abs() < 490.975\n",
    "\n",
    "#             adj = (phi_slope.abs() < self.phi_slope_max) & (z0.abs() < self.z0_max) & (intersected_layer == False)\n",
    "\n",
    "#             row, col = adj.nonzero().t()\n",
    "#             row = nnz1[row]\n",
    "#             col = nnz2[col]\n",
    "#             edge_index = torch.stack([row, col], dim=0)\n",
    "\n",
    "#             edge_indices = torch.cat((edge_indices, edge_index), 1)\n",
    "\n",
    "#         return edge_indices\n",
    "    \n",
    "#     def get(self,idx):\n",
    "        \n",
    "#         hits,particles,truth = self.read_events(idx)   \n",
    "        \n",
    "#         if not self.tracking:\n",
    "#             pos, layer, particle, eta, particle_label, tps = self.select_hits(hits, particles, truth)\n",
    "#             tracks = torch.empty(0, 5, dtype=torch.long)\n",
    "#         else:\n",
    "#             selected_hits,tps = self.select_hits(hits, particles, truth)\n",
    "#             pos, layer, particle, eta, particle_label, tracks = self.build_tracks(selected_hits,\n",
    "#                                                                                   hits, \n",
    "#                                                                                   particles, \n",
    "#                                                                                   truth)      \n",
    "#         phi_edges = np.linspace(*(-np.pi, np.pi), num=self.n_phi_sections+1)\n",
    "#         eta_edges = np.linspace(*self.eta_range, num=self.n_eta_sections+1)\n",
    "        \n",
    "#         pos_sect, layer_sect, particle_sect, particle_label_sect = self.split_detector_sections(pos, layer, particle, eta, particle_label, phi_edges, eta_edges)\n",
    "#         for i in range(len(pos_sect)):\n",
    "#             edge_index = self.compute_edge_index(pos_sect[i], layer_sect[i])\n",
    "#             y = particle_label_sect[i]\n",
    "#             y_particle_barcodes = particle_sect[i]\n",
    "            \n",
    "#             return Data(x=pos_sect[i],\n",
    "#                         y=y,\n",
    "#                         tracks=tracks,\n",
    "#                         inpz = torch.Tensor([idx]))\n",
    "\n",
    "# def fetch_dataloader(data_dir, \n",
    "#                      batch_size, \n",
    "#                      validation_split,\n",
    "#                      n_events = 0,\n",
    "#                      pt_min = 1.0,\n",
    "#                      n_workers = 1,\n",
    "#                      full_dataset = True,\n",
    "#                      shuffle=False):\n",
    "#     volume_layer_ids = [\n",
    "#         [8, 2], [8, 4], [8, 6], [8, 8], # barrel pixels\n",
    "#         [7, 2], [7, 4], [7, 6], [7, 8], [7, 10], [7, 12], [7, 14],# minus pixel endcap\n",
    "#         [9, 2], [9, 4], [9, 6], [9, 8], [9, 10], [9, 12], [9, 14], # plus pixel endcap\n",
    "#     ]\n",
    "#     dataset = TrackMLParticleTrackingDataset(root=data_dir,\n",
    "#                                              layer_pairs_plus=True, \n",
    "#                                              pt_min= pt_min,\n",
    "#                                              volume_layer_ids=volume_layer_ids,\n",
    "#                                              n_events=n_events, \n",
    "#                                              n_workers=n_workers, \n",
    "#                                              tracking = True,\n",
    "#                                              download_full_dataset=full_dataset)\n",
    "#     dataset_size = len(dataset)\n",
    "#     indices = list(range(dataset_size))\n",
    "#     split = int(np.floor(validation_split * dataset_size))\n",
    "#     print(split)\n",
    "#     random_seed= 1001\n",
    "\n",
    "#     train_subset, val_subset = torch.utils.data.random_split(dataset, [dataset_size - split, split],\n",
    "#                                                              generator=torch.Generator().manual_seed(random_seed))\n",
    "#     print(\"train subset dim:\", len(train_subset))\n",
    "#     print(\"validation subset dim\", len(val_subset))\n",
    "#     dataloaders = {\n",
    "#         'train':  DataLoader(train_subset, batch_size=batch_size, shuffle=shuffle),\n",
    "#         'val':   DataLoader(val_subset, batch_size=batch_size, shuffle=shuffle)\n",
    "#         }\n",
    "#     print(\"train_dataloader dim:\", len(dataloaders['train']))\n",
    "#     print(\"val dataloader dim:\", len(dataloaders['val']))\n",
    "#     return dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3180ffc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def assert_no_nans(x):\n",
    "#     \"\"\"\n",
    "#     Raises AssertionError if there is a nan in the tensor\n",
    "#     \"\"\"\n",
    "#     assert not torch.isnan(x).any()\n",
    "\n",
    "# DEBUG = True\n",
    "# def debug(*args, **kwargs):\n",
    "#     if DEBUG: print(*args, **kwargs)\n",
    "        \n",
    "# def batch_cluster_indices(cluster_id: torch.Tensor, batch: torch.Tensor):\n",
    "#     \"\"\"\n",
    "#     Turns cluster indices per event to an index in the whole batch\n",
    "#     Example:\n",
    "#     cluster_id = torch.LongTensor([0, 0, 1, 1, 2, 0, 0, 1, 1, 1, 0, 0, 1])\n",
    "#     batch = torch.LongTensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2])\n",
    "#     -->\n",
    "#     offset = torch.LongTensor([0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 5, 5, 5])\n",
    "#     output = torch.LongTensor([0, 0, 1, 1, 2, 3, 3, 4, 4, 4, 5, 5, 6])\n",
    "#     \"\"\"\n",
    "#     # Count the number of clusters per entry in the batch\n",
    "#     n_clusters_per_event = scatter_max(cluster_id, batch, dim=-1)[0] + 1\n",
    "#     # Offsets are then a cumulative sum\n",
    "#     offset_values_nozero = n_clusters_per_event[:-1].cumsum(dim=-1)\n",
    "#     # Prefix a zero\n",
    "#     offset_values = torch.zeros(offset_values_nozero.size(0)+1)\n",
    "#     offset_values[1:] = offset_values_nozero\n",
    "#     # Fill it per hit\n",
    "#     offset = torch.gather(offset_values, 0, batch).type(torch.LongTensor)\n",
    "#     return offset + cluster_id, n_clusters_per_event\n",
    "\n",
    "# def calc_LV_Lbeta(\n",
    "#     beta: torch.Tensor, cluster_coords: torch.Tensor, # Predicted by model\n",
    "#     cluster_index_per_entry: torch.Tensor, # Truth hit->cluster index\n",
    "#     batch: torch.Tensor,\n",
    "#     qmin = 0.1,\n",
    "#     s_B = 0.1,\n",
    "#     bkg_cluster_index = 0 # cluster_index entries with this value are bkg/noise\n",
    "#     ):\n",
    "#     assert_no_nans(beta)\n",
    "#     assert_no_nans(cluster_coords)\n",
    "#     assert_no_nans(batch)\n",
    "#     assert_no_nans(cluster_index_per_entry)\n",
    "\n",
    "#     n_hits = beta.size(0)\n",
    "#     cluster_space_dim = cluster_coords.size(1)\n",
    "#     # Transform indices-per-event to indices-per-batch\n",
    "#     # E.g. [ 0, 0, 1, 2, 0, 0, 1] -> [ 0, 0, 1, 2, 3, 3, 4 ]\n",
    "#     cluster_index, n_clusters_per_entry = batch_cluster_indices(cluster_index_per_entry, batch)\n",
    "#     n_clusters = cluster_index.max()+1\n",
    "\n",
    "#     debug(f'\\n\\nIn calc_LV_Lbeta; n_hits={n_hits}, n_clusters={n_clusters}')\n",
    "\n",
    "#     q = beta.arctanh()**2 + qmin\n",
    "#     assert_no_nans(q)\n",
    "\n",
    "#     # Select the maximum charge node per cluster\n",
    "#     q_alpha, index_alpha = scatter_max(q, cluster_index) # max q per cluster\n",
    "#     assert index_alpha.size() == (n_clusters,)\n",
    "#     assert_no_nans(q_alpha)\n",
    "    \n",
    "\n",
    "#     debug('beta:', beta)\n",
    "#     debug('q:', q)\n",
    "#     debug('q_alpha:', q_alpha)\n",
    "#     debug('index_alpha:', index_alpha)\n",
    "#     debug('cluster_index:', cluster_index)\n",
    "\n",
    "#     x_alpha = cluster_coords[index_alpha]\n",
    "#     beta_alpha = beta[index_alpha]\n",
    "#     assert_no_nans(x_alpha)\n",
    "#     assert_no_nans(beta_alpha)\n",
    "\n",
    "#     # debug(f'n_hits={n_hits}, n_clusters={n_clusters}, cluster_space_dim={cluster_space_dim}')\n",
    "#     # debug(f'x_alpha.size()={x_alpha.size()}')\n",
    "\n",
    "#     # Copy x_alpha by n_hit rows:\n",
    "#     # (n_clusters x cluster_space_dim) --> (n_hits x n_clusters x cluster_space_dim)\n",
    "#     x_alpha_expanded = x_alpha.expand(n_hits, n_clusters, cluster_space_dim)\n",
    "#     assert_no_nans(x_alpha_expanded)\n",
    "\n",
    "#     # Copy cluster_coord by n_cluster columns:\n",
    "#     # (n_hits x cluster_space_dim) --> (n_hits x n_clusters x cluster_space_dim)\n",
    "#     cluster_coords_expanded = (\n",
    "#         cluster_coords\n",
    "#         .repeat(1,n_clusters).reshape(n_hits, n_clusters, cluster_space_dim)\n",
    "#         )\n",
    "#     assert_no_nans(cluster_coords_expanded)\n",
    "\n",
    "#     # Take the L2 norm; Resulting matrix should be n_hits x n_clusters\n",
    "#     norms = (cluster_coords_expanded - x_alpha_expanded).norm(dim=-1)\n",
    "#     assert norms.size() == (n_hits, n_clusters)\n",
    "#     assert_no_nans(norms)\n",
    "\n",
    "#     # Index to matrix, e.g.:\n",
    "#     # [1, 3, 1, 0] --> [\n",
    "#     #     [0, 1, 0, 0],\n",
    "#     #     [0, 0, 0, 1],\n",
    "#     #     [0, 1, 0, 0],\n",
    "#     #     [1, 0, 0, 0]\n",
    "#     #     ]\n",
    "#     M = torch.nn.functional.one_hot(cluster_index)\n",
    "\n",
    "#     # Copy q_alpha by n_hit rows:\n",
    "#     # (n_clusters) --> (n_hits x n_clusters)\n",
    "#     q_alpha_expanded = q_alpha.expand(n_hits, -1)\n",
    "\n",
    "#     # Potential for hits w.r.t. the cluster they belong to\n",
    "#     V_belonging = M * q_alpha_expanded * norms**2\n",
    "\n",
    "#     # Potential for hits w.r.t. the cluster they DO NOT belong to\n",
    "#     V_notbelonging = 1. - (1-M) * q_alpha_expanded * norms\n",
    "#     V_notbelonging[V_notbelonging < 0.] = 0. # Min of 0\n",
    "\n",
    "#     # Count n_hits per entry in the batch (batch_size)\n",
    "#     _, n_hits_per_entry = torch.unique(batch, return_counts=True)\n",
    "#     # Expand: (batch_size) --> (nhits)\n",
    "#     # e.g. [2, 3, 1] --> [2, 2, 3, 3, 3, 1]\n",
    "#     n_hits_expanded = torch.gather(n_hits_per_entry, 0, batch).type(torch.LongTensor)\n",
    "#     # Alternatively, this should give the same:\n",
    "#     # n_hits_expanded = torch.repeat_interleave(n_hits_per_entry, n_hits_per_entry)\n",
    "\n",
    "#     # Final LV value:\n",
    "#     # (n_hits x 1) * (n_hits x n_clusters) / (n_hits x 1) --> float\n",
    "#     LV = torch.sum(\n",
    "#         q.unsqueeze(-1) * (V_belonging + V_notbelonging) / n_hits_expanded.unsqueeze(-1)\n",
    "#         )\n",
    "#     assert_no_nans(LV)\n",
    "#     # Alternatively:\n",
    "#     # LV = torch.sum(q * (V_belonging + V_notbelonging).sum(dim=-1) / n_hits_expanded)\n",
    "\n",
    "#     # ____________________________________\n",
    "#     # Now calculate Lbeta\n",
    "#     # Lbeta also needs the formatted cluster_index and beta_alpha,\n",
    "#     # both of which are known in this scope, so for now it's easier to \n",
    "#     # calculate it here. Moving it to a dedicated function at some\n",
    "#     # point would be better design.\n",
    "\n",
    "#     is_bkg = cluster_index_per_entry == bkg_cluster_index\n",
    "#     N_B = scatter_add(is_bkg, batch) # (batch_size)\n",
    "#     # Expand (batch_size) -> (n_hits)\n",
    "#     # e.g. [ 3, 2 ], [0, 0, 0, 0, 0, 1, 1, 1, 1] -> [3, 3, 3, 3, 3, 2, 2, 2, 2]\n",
    "#     N_B_expanded = torch.gather(N_B, 0, batch).type(torch.LongTensor)\n",
    "#     bkg_term = s_B * (N_B_expanded*beta)[is_bkg].sum()\n",
    "\n",
    "#     # n_clusters_per_entry: (batch_size)\n",
    "#     # (batch_size) --> (n_clusters)\n",
    "#     # e.g. [3, 2] --> [3, 3, 3, 2, 2]\n",
    "#     n_clusters_expanded = torch.repeat_interleave(n_clusters_per_entry, n_clusters_per_entry)\n",
    "#     assert n_clusters_expanded.size() == (n_clusters,)\n",
    "#     nonbkg_term = ((1.-beta_alpha)/n_clusters_expanded).sum()\n",
    "\n",
    "#     # Final Lbeta\n",
    "#     debug(f'Lp: nonbkg_term={nonbkg_term}, bkg_term={bkg_term}')\n",
    "#     Lbeta = nonbkg_term + bkg_term\n",
    "\n",
    "#     debug(f'LV={LV}, Lbeta={Lbeta}')\n",
    "#     return LV, Lbeta\n",
    "\n",
    "# def softclip(array, start_clip_value):\n",
    "#     array /= start_clip_value\n",
    "#     array = torch.where(array>1, torch.log(array+1.), array)\n",
    "#     return array * start_clip_value\n",
    "\n",
    "# def huber_jan(array, delta):\n",
    "#     \"\"\"\n",
    "#     See: https://en.wikipedia.org/wiki/Huber_loss#Definition\n",
    "#     \"\"\"\n",
    "#     loss_squared = array**2\n",
    "#     array_abs = torch.abs(array)\n",
    "#     loss_linear = delta**2 + 2.*delta * (array_abs - delta)\n",
    "#     return tf.where(array_abs < delta, loss_squared, loss_linear)\n",
    "\n",
    "# def huber(d, delta):\n",
    "#     \"\"\"\n",
    "#     See: https://en.wikipedia.org/wiki/Huber_loss#Definition\n",
    "#     \"\"\"\n",
    "#     return torch.where(d<=delta, .5*d**2, delta*(d-.5*delta))\n",
    "\n",
    "# def calc_L_energy(pred_energy, truth_energy):\n",
    "#     diff = torch.abs(pred_energy - truth_energy)\n",
    "#     L = 10. * torch.exp(-0.1 * diff**2 ) + 0.01*diff\n",
    "#     return softclip(L, 10.)\n",
    "\n",
    "# def calc_L_time(pred_time, truth_time):\n",
    "#     return softclip(huber(torch.abs(pred_time-truth_time), 2.), 6.)\n",
    "\n",
    "# def calc_L_position(pred_position: torch.Tensor, truth_position: torch.Tensor):\n",
    "#     d_squared = ((pred_position-truth_position)**2).sum(dim=-1)\n",
    "#     return softclip(huber(torch.sqrt(d_squared/100. + 1e-2), 10.), 3.)\n",
    "\n",
    "# def calc_L_classification(pred_pid, truth_pid):\n",
    "#     raise NotImplementedError\n",
    "\n",
    "# def calc_Lp(\n",
    "#     pred_beta: torch.Tensor, truth_cluster_index,\n",
    "#     pred_cluster_properties, truth_cluster_properties\n",
    "#     ):\n",
    "#     \"\"\"\n",
    "#     Property loss\n",
    "#     Assumes:\n",
    "#     0 : energy,\n",
    "#     1 : time,\n",
    "#     2,3 : boundary crossing position,\n",
    "#     4 : pdgid\n",
    "#     \"\"\"\n",
    "#     xi = torch.zeros_like(pred_beta)\n",
    "#     xi[truth_cluster_index > 0] = pred_beta[truth_cluster_index > 0].arctanh()\n",
    "\n",
    "#     L_energy = calc_L_energy(pred_cluster_properties[:,0], truth_cluster_properties[:,0])\n",
    "#     L_time = calc_L_time(pred_cluster_properties[:,1], truth_cluster_properties[:,1])\n",
    "#     L_position = calc_L_position(pred_cluster_properties[:,2:4], truth_cluster_properties[:,2:4])\n",
    "#     # L_classification = calc_L_classification(pred_cluster_properties[:,4], pred_cluster_properties[:,4]) TODO\n",
    "\n",
    "#     Lp = 0\n",
    "#     xi_sum = xi.sum()\n",
    "#     for L in [ L_energy, L_time, L_position ]:\n",
    "#         Lp += 1./xi_sum * (xi * L).sum()\n",
    "# #     print(f'Lp={Lp}')\n",
    "#     return Lp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07ba4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_oc_loss(out, \n",
    "                    data, \n",
    "                    s_c=1.):\n",
    "    pred_betas = torch.sigmoid(out[:,0])\n",
    "    pred_cluster_space_coords = out[:,1:3]\n",
    "    pred_cluster_properties = out[:,3:]\n",
    "    LV, Lbeta = calc_LV_Lbeta(\n",
    "        pred_betas,\n",
    "        pred_cluster_space_coords,\n",
    "        data.y.type(torch.LongTensor),\n",
    "        data.batch\n",
    "        )\n",
    "    Lp = calc_Lp(\n",
    "        pred_betas,\n",
    "        data.y.type(torch.LongTensor),\n",
    "        pred_cluster_properties,\n",
    "        data.tracks\n",
    "        )\n",
    "    return Lp + s_c*(LV + Lbeta)\n",
    "\n",
    "def main():\n",
    "#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#     print('Using device', device)\n",
    "#    root = \"/Users/gpradhan/Downloads/train_1\"\n",
    "    root = \"/home/gpradhan/trackML/train\"\n",
    "    batch_size = 4\n",
    "    validation_split = 0.2\n",
    "    data = fetch_dataloader(data_dir = root,\n",
    "                            batch_size = batch_size,\n",
    "                            validation_split=validation_split,\n",
    "                            full_dataset = False,\n",
    "                            n_events = 2000,\n",
    "                            pt_min = 0.0,\n",
    "                            shuffle=True)\n",
    "    train_loader, test_loader = data['train'],data['val']\n",
    "    model = GravnetModel(input_dim=3, output_dim=8)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "    \n",
    "    def train(epoch):\n",
    "        print('Training epoch', epoch)\n",
    "        model.train()\n",
    "        pbar = tqdm.tqdm(train_loader, total=len(train_loader))\n",
    "        pbar.set_postfix({'loss': '?'})\n",
    "        for data in pbar:\n",
    "#            data = data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            result = model(data.x, data.batch)\n",
    "            loss = compute_oc_loss(result, data)\n",
    "            print(f'loss={float(loss)}')\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            pbar.set_postfix({'loss': float(loss)})\n",
    "            \n",
    "    def test(epoch):\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            loss = 0.\n",
    "            for data in tqdm.tqdm(test_loader, total=len(test_loader)):\n",
    "#                data = data.to(device)\n",
    "                result = model(data.x, data.batch)\n",
    "                loss += compute_oc_loss(result, data)\n",
    "            loss /= len(test_loader)\n",
    "            print(f'Avg test loss: {loss}')\n",
    "     \n",
    "    for i_epoch in range(1):\n",
    "        train(i_epoch)\n",
    "        test(i_epoch)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1837dedc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
